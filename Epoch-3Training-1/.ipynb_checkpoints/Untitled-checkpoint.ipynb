{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82fb0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset.\n",
    "## Go to the dataset directory.\n",
    "%cd dataset/\n",
    "!ls\n",
    "## Import the pandas library and perform data reading for analysis.\n",
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv', sep='\\t', header=None)\n",
    "test = pd.read_csv('test.csv', sep='\\t', header=None)\n",
    "## Adding column names to data.\n",
    "train.columns = [\"text\",'labels']\n",
    "test.columns = [\"text\",'labels']\n",
    "## The first 10 texts of the training set are read in the format \"text, labels\". The labels may contain multiple sentiment categories, each sentiment is separated by ','.\n",
    "train.head(10)\n",
    "test.head(10)\n",
    "# Install and import related libraries.\n",
    "!pip install --upgrade paddlenlp\n",
    "!pip install paddlepaddle\n",
    "import os\n",
    "import paddle\n",
    "import paddlenlp\n",
    "# Data pre-processing\n",
    "## For the 28 micro-sentiment multi-label classification scenario, where a sentence may correspond to multiple sentiment category labels, the sentiment labels of the dataset need to be transformed using One-Hot coding first, with \"0\" indicating absence and \"1\" indicating presence for each sentiment.\n",
    "### Create sentiment label mapping relationships.\n",
    "label_vocab = {\n",
    "    0: \"admiration\",\n",
    "    1: \"amusement\",\n",
    "    2: \"anger\",\n",
    "    3: \"annoyance\",\n",
    "    4: \"approval\",\n",
    "    5: \"caring\",\n",
    "    6: \"confusion\",\n",
    "    7: \"curiosity\",\n",
    "    8: \"desire\",\n",
    "    9: \"disappointment\",\n",
    "    10: \"disapproval\",\n",
    "    11: \"disgust\",\n",
    "    12: \"embarrassment\",\n",
    "    13: \"excitement\",\n",
    "    14: \"fear\",\n",
    "    15: \"gratitude\",\n",
    "    16: \"grief\",\n",
    "    17: \"joy\",\n",
    "    18: \"love\",\n",
    "    19: \"nervousness\",\n",
    "    20: \"optimism\",\n",
    "    21: \"pride\",\n",
    "    22: \"realization\",\n",
    "    23: \"relief\",\n",
    "    24: \"remorse\",\n",
    "    25: \"sadness\",\n",
    "    26: \"surprise\",\n",
    "    27: \"neutral\"\n",
    "}\n",
    "### Customize the dataset, read the data file, create the dataset and define the data type as MapDataset.\n",
    "import re\n",
    "\n",
    "from paddlenlp.datasets import load_dataset\n",
    "\n",
    "# Clear invalid characters\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
    "    text = re.sub(r\"\\\\n\\n\", \".\", text)\n",
    "    return text\n",
    "\n",
    "# Define the read data set function\n",
    "def read_custom_data(filepath, is_one_hot=True):\n",
    "    f = open(filepath)\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        data = line.strip().split('\\t')\n",
    "        # One-hot processing for 28 types of micro sentiment tags\n",
    "        if is_one_hot:\n",
    "            labels = [float(1) if str(i) in data[1].split(',') else float(0) for i in range(28)]  # 28 types\n",
    "        else:\n",
    "            labels = [int(d) for d in data[1].split(',')]\n",
    "        yield {\"text\": clean_text(data[0]), \"labels\": labels}\n",
    "    f.close()\n",
    "# load_dataset() to Create dataset.\n",
    "# lazy=False，The dataset is returned as a MapDataset type.\n",
    "# Pre-processing of training and validation sets.\n",
    "train_ds = load_dataset(read_custom_data, filepath='train.csv', lazy=False) \n",
    "test_ds = load_dataset(read_custom_data, filepath='test.csv', lazy=False)\n",
    "### Print dataset.\n",
    "print(\"datatype:\", type(train_ds))\n",
    "print(\"training dataset example:\", train_ds[0])\n",
    "print(\"testing dataset example:\", test_ds[0])\n",
    "## Load Chinese ERNIE 3.0 pre-training model and word splitter\n",
    "from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"ernie-3.0-medium-zh\"   # ERNIE3.0 model\n",
    "num_classes = 28  # 28 classification mission\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_classes=num_classes)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "## Process the raw data into a model-acceptable format.\n",
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "from paddle.io import DataLoader, BatchSampler\n",
    "from paddlenlp.data import DataCollatorWithPadding\n",
    "\n",
    "# Data pre-processing function to convert text into integer sequences using a word splitter.\n",
    "def preprocess_function(examples, tokenizer, max_seq_length):\n",
    "    result = tokenizer(text=examples[\"text\"], max_seq_len=max_seq_length)\n",
    "    result[\"labels\"] = examples[\"labels\"]\n",
    "    return result\n",
    "\n",
    "trans_func = functools.partial(preprocess_function, tokenizer=tokenizer, max_seq_length=64)\n",
    "train_ds = train_ds.map(trans_func)\n",
    "test_ds = test_ds.map(trans_func)\n",
    "\n",
    "# function is constructed to extend the different length sequences to the maximum length of the data in the batch, and then stack the data.\n",
    "collate_fn = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# Define the BatchSampler, select the batch size and whether to randomly jumble the DataLoader.\n",
    "train_batch_sampler = BatchSampler(train_ds, batch_size=32, shuffle=True)\n",
    "test_batch_sampler = BatchSampler(test_ds, batch_size=16, shuffle=False)\n",
    "train_data_loader = DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(dataset=test_ds, batch_sampler=test_batch_sampler, collate_fn=collate_fn)\n",
    "## Define model validation metrics.\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from paddle.metric import Metric\n",
    "\n",
    "# Customize MultiLabelReport evaluation metrics.\n",
    "class MultiLabelReport(Metric):\n",
    "    \"\"\"\n",
    "    AUC and F1 Score for multi-label text classification task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name='MultiLabelReport', average='micro'):\n",
    "        super(MultiLabelReport, self).__init__()\n",
    "        self.average = average\n",
    "        self._name = name\n",
    "        self.reset()\n",
    "\n",
    "    def f1_score(self, y_prob):\n",
    "        '''\n",
    "        Returns the f1 score by searching the best threshhold\n",
    "        '''\n",
    "        best_score = 0\n",
    "        for threshold in [i * 0.01 for i in range(100)]:\n",
    "            self.y_pred = y_prob > threshold\n",
    "            score = sklearn.metrics.f1_score(y_pred=self.y_pred, y_true=self.y_true, average=self.average)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                precison = precision_score(y_pred=self.y_pred, y_true=self.y_true, average=self.average)\n",
    "                recall = recall_score(y_pred=self.y_pred, y_true=self.y_true, average=self.average)\n",
    "        return best_score, precison, recall\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets all of the metric state.\n",
    "        \"\"\"\n",
    "        self.y_prob = None\n",
    "        self.y_true = None\n",
    "\n",
    "    def update(self, probs, labels):\n",
    "        if self.y_prob is not None:\n",
    "            self.y_prob = np.append(self.y_prob, probs.numpy(), axis=0)\n",
    "        else:\n",
    "            self.y_prob = probs.numpy()\n",
    "        if self.y_true is not None:\n",
    "            self.y_true = np.append(self.y_true, labels.numpy(), axis=0)\n",
    "        else:\n",
    "            self.y_true = labels.numpy()\n",
    "\n",
    "    def accumulate(self):\n",
    "        auc = roc_auc_score(\n",
    "            y_score=self.y_prob, y_true=self.y_true, average=self.average)\n",
    "        f1_score, precison, recall = self.f1_score(y_prob=self.y_prob)\n",
    "        return auc, f1_score, precison, recall\n",
    "\n",
    "    def name(self):\n",
    "        \"\"\"\n",
    "        Returns metric name\n",
    "        \"\"\"\n",
    "        return self._name\n",
    "# Building the training model.\n",
    "## Select an optimization strategy and run configuration.\n",
    "import time\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# AdamW optimizer, cross-entropy loss function, custom MultiLabelReport evaluation metrics.\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=4e-5, parameters=model.parameters(), weight_decay=0.01)\n",
    "criterion = paddle.nn.BCEWithLogitsLoss()\n",
    "metric = MultiLabelReport()\n",
    "## Model training and validation.\n",
    "import paddle\n",
    "import numpy as np\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# Build the validation set evaluate function.\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader, label_vocab, if_return_results=True):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    results = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.sigmoid(logits)\n",
    "        losses.append(loss.numpy())\n",
    "        metric.update(probs, labels)\n",
    "        if if_return_results:\n",
    "            probs = probs.tolist()\n",
    "            for prob in probs:\n",
    "                result = []\n",
    "                for c, pred in enumerate(prob):\n",
    "                    if pred > 0.5:\n",
    "                        result.append(label_vocab[c])\n",
    "                results.append(','.join(result))\n",
    "\n",
    "    auc, f1_score, precison, recall = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, auc: %.5f, f1 score: %.5f, precison: %.5f, recall: %.5f\" %\n",
    "          (np.mean(losses), auc, f1_score, precison, recall))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    if if_return_results:\n",
    "        return results\n",
    "    else:\n",
    "        return f1_score\n",
    "epochs = 3 # training times\n",
    "ckpt_dir = \"ernie_ckpt\" # Folder for saving model parameters during training\n",
    "\n",
    "global_step = 0  # Number of iterations\n",
    "tic_train = time.time()\n",
    "best_f1_score = 0\n",
    "\n",
    "# Model Training\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
    "\n",
    "        # Calculate model output, loss function value, classification probability value, accuracy, f1 score.\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.sigmoid(logits)\n",
    "        metric.update(probs, labels)\n",
    "        auc, f1_score, _, _ = metric.accumulate()\n",
    "\n",
    "        # Print the loss function value, accuracy, f1 score, and computation speed for each 10 iterations.\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, auc: %.5f, f1 score: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, auc, f1_score,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "        \n",
    "        # Reverse gradient passback with updated parameters.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "        \n",
    "        # Every 40 iterations, evaluate the current trained model, save the current best model parameters and word list of the word splitter, etc.\n",
    "        if global_step % 40 == 0:\n",
    "            save_dir = ckpt_dir\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            eval_f1_score = evaluate(model, criterion, metric, test_data_loader, label_vocab, if_return_results=False)\n",
    "            if eval_f1_score > best_f1_score:\n",
    "                best_f1_score = eval_f1_score\n",
    "                model.save_pretrained(save_dir)\n",
    "                tokenizer.save_pretrained(save_dir)\n",
    "## Model Validation Performance Results\n",
    "# Load the optimal parameters of the trained model.\n",
    "model.set_dict(paddle.load('ernie_ckpt/model_state.pdparams'))\n",
    "\n",
    "# Load the parameters of the previously trained model.\n",
    "# model.set_dict(paddle.load('/home/aistudio/work/model_state.pdparams'))\n",
    "\n",
    "# Model Validation.\n",
    "print(\"ERNIE 3.0 performance on GoEmotions micro-emotion 28 classification test set：\", end= \" \")\n",
    "results = evaluate(model, criterion, metric, test_data_loader, label_vocab)\n",
    "## 28 Multi-Label Groups of \"Micro\" Emotions Predicting Demo\n",
    "# Define data loading and processing functions\n",
    "from paddlenlp.data import JiebaTokenizer, Pad, Stack, Tuple, Vocab\n",
    "def convert_example(example, tokenizer, max_seq_length=64, is_test=False):\n",
    "    qtconcat = example[\"text\"]\n",
    "    encoded_inputs = tokenizer(text=qtconcat, max_seq_len=max_seq_length)\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "    if not is_test:\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        return input_ids, token_type_ids\n",
    "\n",
    "# Define the model prediction function\n",
    "def predict(model, data, tokenizer, label_vocab, batch_size=1, max_seq=64):\n",
    "    examples = []\n",
    "    # Process input data (the list form) into a format acceptable to the model\n",
    "    for text in data:\n",
    "        input_ids, segment_ids = convert_example(\n",
    "            text,\n",
    "            tokenizer,\n",
    "            max_seq_length=max_seq,\n",
    "            is_test=True)\n",
    "        examples.append((input_ids, segment_ids))\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input id\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # segment id\n",
    "    ): fn(samples)\n",
    "\n",
    "    # Seperates data into some batches.\n",
    "    batches = []\n",
    "    one_batch = []\n",
    "    for example in examples:\n",
    "        one_batch.append(example)\n",
    "        if len(one_batch) == batch_size:\n",
    "            batches.append(one_batch)\n",
    "            one_batch = []\n",
    "    if one_batch:\n",
    "        # The last batch whose size is less than the config batch_size setting.\n",
    "        batches.append(one_batch)\n",
    "\n",
    "    results = []\n",
    "    model.eval()\n",
    "    for batch in batches:\n",
    "        input_ids, segment_ids = batchify_fn(batch)\n",
    "        input_ids = paddle.to_tensor(input_ids)\n",
    "        segment_ids = paddle.to_tensor(segment_ids)\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        probs = F.sigmoid(logits)\n",
    "        probs = probs.tolist()\n",
    "        # The results were processed by selecting the sentiment categories with probability greater than 0.5\n",
    "        for prob in probs:\n",
    "            result = []\n",
    "            for c, pred in enumerate(prob):\n",
    "                if pred > 0.5:\n",
    "                    result.append(label_vocab[c])\n",
    "            results.append(','.join(result))\n",
    "    return results  # Return the predeicted results\n",
    "## Prediction Results\n",
    "# Define the text data to be subjected to micro-sentiment analysis\n",
    "data = [\n",
    "    # 0 admiration\n",
    "    {\"text\": 'You do a great job!'},\n",
    "    # 1 amusement\n",
    "    {\"text\": 'Lets have fun'},\n",
    "    # 2 anger\n",
    "    {\"text\":\"You shut your mouth\"},\n",
    "    # 3 annoyance\n",
    "    {\"text\": 'You are so annoyed'},\n",
    "    # 4 approval\n",
    "    {\"text\": 'You are allowed to do this'},\n",
    "    # 5 caring & # 7 Curiosity\n",
    "    {\"text\": 'Are you feeling well?'},\n",
    "    # 6 confusion\n",
    "    {\"text\": 'This problem is so hard and I cannot solve this problem'},\n",
    "    # 7 curiosity\n",
    "    {\"text\":'Why would I do that?'},\n",
    "    # 8 desire\n",
    "    {\"text\": 'I want this gift so much'},\n",
    "    # 9 disappointment\n",
    "    {\"text\": 'I am very disappointed by everything you have done to me'},\n",
    "    # 10 disapproval\n",
    "    {\"text\": 'You are not admitted to the college.'},\n",
    "    # 11 disgust\n",
    "    {\"text\": 'Thats absolutely disgusting.'},\n",
    "    # 12 embarrassment\n",
    "    {\"text\": 'Thats so embarrassing.'},\n",
    "    # 13 excitement\n",
    "    {\"text\": 'I am so excited'},\n",
    "    # 14 fear\n",
    "    {\"text\": 'I am so scared of skydiving'},\n",
    "    # 15 gratitude\n",
    "    {\"text\":\"Thank you.\"},\n",
    "    # 16 grief\n",
    "    {\"text\": 'My grandpa passed away'},\n",
    "    # 17 joy\n",
    "    {\"text\": 'Happy Birthday'},\n",
    "    # 18 love\n",
    "    {\"text\": 'I love you so much'},\n",
    "    # 19 nervousness\n",
    "    {\"text\": 'I am so nervous.'},\n",
    "    # 20 neutral\n",
    "    {\"text\": 'It is just so so.'},\n",
    "    # 21 optimism\n",
    "    {\"text\": 'Successful people only focus on giving their best effort.'},\n",
    "    # 22 pride\n",
    "    {\"text\": 'I am so proud of you.'},\n",
    "    # 23 realization\n",
    "    {\"text\": 'Thank you for letting me realizing this rule.'},\n",
    "    # 24 relief \n",
    "    {\"text\": 'You are doing better than you think you are'},\n",
    "    # 25 remorse\n",
    "    {\"text\": 'I am guilty.'},\n",
    "    # 26 sadness\n",
    "    {\"text\": 'I am so sad.'},\n",
    "    # 27 surprise\n",
    "    {\"text\": 'I am so surprised that you made it.'},\n",
    "    \n",
    "]\n",
    "\n",
    "# Model Prediction\n",
    "labels =  predict(model, data, tokenizer, label_vocab, batch_size=1)\n",
    "\n",
    "# Output of predicted results\n",
    "for idx, text in enumerate(data):\n",
    "    print('Text: {} \\t Labels: {}'.format(text['text'], labels[idx]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
